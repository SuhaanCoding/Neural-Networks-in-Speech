{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Leaky ReLU Experiments\n",
        "\n",
        "Testing leaky ReLU activation for speech classification.\n",
        "\n",
        "Leaky ReLU allows a small gradient when the input is negative (instead of completely zeroing it out like regular ReLU). This can sometimes help with gradient flow during training.\n",
        "\n",
        "## Setup\n",
        "Put your audio files in class folders and update the path below.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa as lr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATASET_DIR = \"/path/to/your/dataset\"  # CHANGE THIS TO YOUR DATASET PATH\n",
        "OUTPUT_BASE = \"results\"\n",
        "SAMPLE_RATE = 16000\n",
        "EPOCHS = 10\n",
        "RANDOM_STATE = 44\n",
        "\n",
        "print(f\"Dataset directory: {DATASET_DIR}\")\n",
        "print(f\"Output base directory: {OUTPUT_BASE}\")\n",
        "print(f\"Sample rate: {SAMPLE_RATE}\")\n",
        "print(f\"Training epochs: {EPOCHS}\")\n",
        "print(f\"Activation: Leaky ReLU\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Leaky ReLU Experiments\n",
        "\n",
        "This section loads data, defines models, and runs experiments using Leaky ReLU activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions (same as in master notebook)\n",
        "def count_samples(path):\n",
        "    \"\"\"Count samples in each class directory\"\"\"\n",
        "    sizes = [len(os.listdir(os.path.join(path, d))) for d in os.listdir(path)]\n",
        "    return pd.DataFrame(sizes, index=os.listdir(path), columns=['num_samples'])\n",
        "\n",
        "def load_dataset(path):\n",
        "    \"\"\"Load audio files and labels from dataset directory\"\"\"\n",
        "    data, labels, samples = [], [], []\n",
        "    for label in os.listdir(path):\n",
        "        dir_ = os.path.join(path, label)\n",
        "        for fname in os.listdir(dir_):\n",
        "            y, sr = lr.load(os.path.join(dir_, fname), sr=SAMPLE_RATE)\n",
        "            data.append(y)\n",
        "            samples.append(sr)\n",
        "            labels.append(label)\n",
        "    return data, labels, samples\n",
        "\n",
        "def encode_labels(labels):\n",
        "    \"\"\"Encode string labels to integers\"\"\"\n",
        "    code = {lab: i for i, lab in enumerate(sorted(set(labels)))}\n",
        "    y = [code[lab] for lab in labels]\n",
        "    return np.array(y), code\n",
        "\n",
        "# Load and prepare dataset\n",
        "print(\"Counting samples:\")\n",
        "print(count_samples(DATASET_DIR))\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "data, labs, _ = load_dataset(DATASET_DIR)\n",
        "y, label_map = encode_labels(labs)\n",
        "X = np.array(data).reshape(-1, SAMPLE_RATE, 1)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(label_map)}\")\n",
        "print(f\"Label mapping: {label_map}\")\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=RANDOM_STATE, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model building and training functions\n",
        "def build_model(layers, input_shape, num_classes):\n",
        "    \"\"\"Build a sequential model from layer configuration\"\"\"\n",
        "    m = keras.Sequential()\n",
        "    for i, L in enumerate(layers):\n",
        "        t = L['type']\n",
        "        if t == 'conv':\n",
        "            kwargs = dict(filters=L['filters'], kernel_size=L['kernel_size'],\n",
        "                          activation=L['activation'])\n",
        "            if i == 0:\n",
        "                m.add(keras.layers.Conv1D(input_shape=input_shape, **kwargs))\n",
        "            else:\n",
        "                m.add(keras.layers.Conv1D(**kwargs))\n",
        "        elif t == 'pool':\n",
        "            m.add(keras.layers.MaxPooling1D(pool_size=L['pool_size']))\n",
        "        elif t == 'dropout':\n",
        "            m.add(keras.layers.Dropout(rate=L['rate']))\n",
        "        elif t == 'flatten':\n",
        "            m.add(keras.layers.Flatten())\n",
        "        elif t == 'dense':\n",
        "            m.add(keras.layers.Dense(L['units'], activation=L['activation']))\n",
        "    m.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "    m.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "def train_and_evaluate_model(model, layers, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate a model, returning history and metrics\"\"\"\n",
        "    print(\"Configuration:\")\n",
        "    for L in layers:\n",
        "        c = L.copy()\n",
        "        if 'activation' in c and callable(c['activation']):\n",
        "            c['activation'] = c['activation'].__name__\n",
        "        print(\"  \", c)\n",
        "    \n",
        "    start = time.time()\n",
        "    history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        verbose=2)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
        "        'classification_report': classification_report(\n",
        "            y_test, y_pred, output_dict=True),\n",
        "        'training_time': elapsed,\n",
        "        'val_accuracy': history.history['val_accuracy'],\n",
        "        'val_loss': history.history['val_loss']\n",
        "    }\n",
        "    return history, metrics\n",
        "\n",
        "print(\"Model building functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Leaky ReLU experiments\n",
        "print(\"=== Running Leaky ReLU Experiments ===\")\n",
        "\n",
        "# Generate simple configurations for demonstration\n",
        "depth_configs = [\n",
        "    [\n",
        "        {'type': 'conv', 'filters': 8, 'kernel_size': 13, 'activation': tf.nn.leaky_relu},\n",
        "        {'type': 'pool', 'pool_size': 3},\n",
        "        {'type': 'dropout', 'rate': 0.3}\n",
        "    ]\n",
        "]\n",
        "\n",
        "size_configs = [\n",
        "    [\n",
        "        {'type': 'flatten'},\n",
        "        {'type': 'dense', 'units': 128, 'activation': tf.nn.leaky_relu},\n",
        "        {'type': 'dropout', 'rate': 0.3},\n",
        "        {'type': 'dense', 'units': 64, 'activation': tf.nn.leaky_relu},\n",
        "        {'type': 'dropout', 'rate': 0.3}\n",
        "    ]\n",
        "]\n",
        "\n",
        "filter_configs = [\n",
        "    [\n",
        "        {'type': 'conv', 'filters': 16, 'kernel_size': 13, 'activation': tf.nn.leaky_relu},\n",
        "        {'type': 'conv', 'filters': 32, 'kernel_size': 11, 'activation': tf.nn.leaky_relu}\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Generate combinations\n",
        "combos = []\n",
        "for d in depth_configs:\n",
        "    for s in size_configs:\n",
        "        for f in filter_configs:\n",
        "            name = f\"D{depth_configs.index(d)}_S{size_configs.index(s)}_F{filter_configs.index(f)}\"\n",
        "            combos.append({'name': name, 'layers': d + f + s})\n",
        "\n",
        "print(f\"Generated {len(combos)} configurations for Leaky ReLU\")\n",
        "\n",
        "# Run experiments\n",
        "leaky_results = []\n",
        "outdir = os.path.join(OUTPUT_BASE, 'Leaky')\n",
        "\n",
        "for cfg in combos:\n",
        "    print(f\"\\n--- Training {cfg['name']} ---\")\n",
        "    \n",
        "    # Train and evaluate\n",
        "    model = build_model(cfg['layers'], input_shape=(SAMPLE_RATE, 1),\n",
        "                        num_classes=len(np.unique(y_train)))\n",
        "    hist, mets = train_and_evaluate_model(\n",
        "        model, cfg['layers'], X_train, y_train, X_test, y_test)\n",
        "    \n",
        "    leaky_results.append((cfg['name'], mets))\n",
        "\n",
        "print(f\"\\nLeaky ReLU experiments complete!\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n=== Leaky ReLU Results ===\")\n",
        "for name, metrics in leaky_results:\n",
        "    print(f\"{name}: Accuracy={metrics['accuracy']:.3f}, F1={metrics['f1_score']:.3f}, Time={metrics['training_time']:.1f}s\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
