{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Testing ReLU Activation\n",
    "\n",
    "Quick experiment to test ReLU activation on speech commands.\n",
    "\n",
    "ReLU is probably the most common activation function - it just zeros out negative values and passes positive values through unchanged.\n",
    "\n",
    "## Data format\n",
    "```\n",
    "data/\n",
    "├── yes/\n",
    "├── no/\n",
    "└── up/\n",
    "```\n",
    "\n",
    "Run the cells below to train and test a CNN with ReLU activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa as lr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(\"Activation: ReLU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_DIR = \"/path/to/your/dataset\"  # CHANGE THIS TO YOUR DATASET PATH\n",
    "OUTPUT_BASE = \"results\"\n",
    "SAMPLE_RATE = 16000\n",
    "EPOCHS = 10\n",
    "RANDOM_STATE = 44\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Output base directory: {OUTPUT_BASE}\")\n",
    "print(f\"Sample rate: {SAMPLE_RATE}\")\n",
    "print(f\"Training epochs: {EPOCHS}\")\n",
    "print(f\"Activation: ReLU\")\n",
    "\n",
    "# Helper functions and data loading (same as master notebook)\n",
    "def count_samples(path):\n",
    "    sizes = [len(os.listdir(os.path.join(path, d))) for d in os.listdir(path)]\n",
    "    return pd.DataFrame(sizes, index=os.listdir(path), columns=['num_samples'])\n",
    "\n",
    "def load_dataset(path):\n",
    "    data, labels, samples = [], [], []\n",
    "    for label in os.listdir(path):\n",
    "        dir_ = os.path.join(path, label)\n",
    "        for fname in os.listdir(dir_):\n",
    "            y, sr = lr.load(os.path.join(dir_, fname), sr=SAMPLE_RATE)\n",
    "            data.append(y)\n",
    "            samples.append(sr)\n",
    "            labels.append(label)\n",
    "    return data, labels, samples\n",
    "\n",
    "def encode_labels(labels):\n",
    "    code = {lab: i for i, lab in enumerate(sorted(set(labels)))}\n",
    "    y = [code[lab] for lab in labels]\n",
    "    return np.array(y), code\n",
    "\n",
    "def build_model(layers, input_shape, num_classes):\n",
    "    m = keras.Sequential()\n",
    "    for i, L in enumerate(layers):\n",
    "        t = L['type']\n",
    "        if t == 'conv':\n",
    "            kwargs = dict(filters=L['filters'], kernel_size=L['kernel_size'],\n",
    "                          activation=L['activation'])\n",
    "            if i == 0:\n",
    "                m.add(keras.layers.Conv1D(input_shape=input_shape, **kwargs))\n",
    "            else:\n",
    "                m.add(keras.layers.Conv1D(**kwargs))\n",
    "        elif t == 'pool':\n",
    "            m.add(keras.layers.MaxPooling1D(pool_size=L['pool_size']))\n",
    "        elif t == 'dropout':\n",
    "            m.add(keras.layers.Dropout(rate=L['rate']))\n",
    "        elif t == 'flatten':\n",
    "            m.add(keras.layers.Flatten())\n",
    "        elif t == 'dense':\n",
    "            m.add(keras.layers.Dense(L['units'], activation=L['activation']))\n",
    "    m.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    m.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def train_and_evaluate_model(model, layers, X_train, y_train, X_test, y_test):\n",
    "    print(\"Configuration:\")\n",
    "    for L in layers:\n",
    "        c = L.copy()\n",
    "        if 'activation' in c and callable(c['activation']):\n",
    "            c['activation'] = c['activation'].__name__\n",
    "        print(\"  \", c)\n",
    "    \n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
    "                        validation_data=(X_test, y_test), verbose=2)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'training_time': elapsed\n",
    "    }\n",
    "    return history, metrics\n",
    "\n",
    "print(\"Functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "print(\"Loading dataset...\")\n",
    "data, labs, _ = load_dataset(DATASET_DIR)\n",
    "y, label_map = encode_labels(labs)\n",
    "X = np.array(data).reshape(-1, SAMPLE_RATE, 1)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(label_map)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=RANDOM_STATE, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Run ReLU experiments\n",
    "print(\"\\n=== Running ReLU Experiments ===\")\n",
    "\n",
    "layers = [\n",
    "    {'type': 'conv', 'filters': 16, 'kernel_size': 13, 'activation': tf.nn.relu},\n",
    "    {'type': 'pool', 'pool_size': 3},\n",
    "    {'type': 'dropout', 'rate': 0.3},\n",
    "    {'type': 'conv', 'filters': 32, 'kernel_size': 11, 'activation': tf.nn.relu},\n",
    "    {'type': 'pool', 'pool_size': 3},\n",
    "    {'type': 'dropout', 'rate': 0.3},\n",
    "    {'type': 'flatten'},\n",
    "    {'type': 'dense', 'units': 128, 'activation': tf.nn.relu},\n",
    "    {'type': 'dropout', 'rate': 0.3},\n",
    "    {'type': 'dense', 'units': 64, 'activation': tf.nn.relu},\n",
    "    {'type': 'dropout', 'rate': 0.3}\n",
    "]\n",
    "\n",
    "model = build_model(layers, input_shape=(SAMPLE_RATE, 1), num_classes=len(np.unique(y_train)))\n",
    "history, metrics = train_and_evaluate_model(model, layers, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(f\"\\nReLU Results:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics['f1_score']:.3f}\")\n",
    "print(f\"  Training Time: {metrics['training_time']:.1f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
